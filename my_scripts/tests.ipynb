{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.basic as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from time import time, ctime\n",
    "from datetime import datetime\n",
    "from utils import construct_mol, correct_mol, set_random_seed\n",
    "from envs import environment as env\n",
    "from models.graphflow import squeeze_adj\n",
    "from models.MolHF import MolHF\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import PretrainDataset\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобученная модель для генерации\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.dataset = 'zinc250k' \n",
    "parser.device = 'cuda' \n",
    "parser.deq_scale = 0.6 \n",
    "parser.batch_size = 256\n",
    "parser.lr = 1e-3 \n",
    "parser.squeeze_fold = 2 \n",
    "parser.n_block = 4 \n",
    "parser.a_num_flows = 6 \n",
    "parser.num_layers = 2 \n",
    "parser.hid_dim = 256 \n",
    "parser.b_num_flows = 3 \n",
    "parser.filter_size = 256 \n",
    "parser.temperature = 0.6 \n",
    "parser.learn_prior = True \n",
    "parser.inv_conv = True \n",
    "parser.inv_rotate = True \n",
    "parser.condition = True \n",
    "parser.init_checkpoint = './save_pretrain/zinc250k_model/checkpoint.pth' \n",
    "parser.gen_num = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предобученная модель для оптимизации\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.dataset = 'zinc250k' \n",
    "parser.device = 'cuda' \n",
    "parser.deq_scale = 0.6 \n",
    "parser.batch_size = 256\n",
    "parser.lr = 1e-3 \n",
    "parser.squeeze_fold = 2 \n",
    "parser.n_block = 4 \n",
    "parser.a_num_flows = 6 \n",
    "parser.num_layers = 2 \n",
    "parser.hid_dim = 256 \n",
    "parser.b_num_flows = 3 \n",
    "parser.filter_size = 256 \n",
    "parser.temperature = 0.6 \n",
    "parser.learn_prior = True \n",
    "parser.inv_conv = True \n",
    "parser.inv_rotate = True \n",
    "parser.condition = True \n",
    "parser.init_checkpoint = './save_pretrain/zinc250k_model/checkpoint.pth' \n",
    "parser.topk = 30 \n",
    "parser.num_iter = 10 \n",
    "parser.opt_lr = 0.5 \n",
    "parser.consopt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser\n",
    "set_random_seed(args.seed)\n",
    "if args.save:\n",
    "    dt = datetime.now()\n",
    "    # TODO: Add more information.\n",
    "    log_dir = os.path.join('./save_pretrain', args.model, args.order, '{}_{:02d}-{:02d}-{:02d}'.format(\n",
    "        dt.date(), dt.hour, dt.minute, dt.second))\n",
    "    args.save_path = log_dir\n",
    "\n",
    "    if not os.path.exists(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "\n",
    "if args.dataset == 'polymer':\n",
    "    # polymer\n",
    "    num2atom = {0: 6, 1: 7, 2: 8, 3: 9, 4: 14, 5: 15, 6: 16}\n",
    "    atom_valency = {6: 4, 7: 3, 8: 2, 9: 1, 14: 4, 15: 3, 16: 2}\n",
    "else:\n",
    "    # zinc250k\n",
    "    num2atom = {0: 6, 1: 7, 2: 8, 3: 9, 4: 15, 5: 16, 6: 17, 7: 35, 8: 53}\n",
    "    atom_valency = {6: 4, 7: 3, 8: 2, 9: 1,\n",
    "                    15: 3, 16: 2, 17: 1, 35: 1, 53: 1}\n",
    "\n",
    "# load data\n",
    "data_path = os.path.join('./data_preprocessed', args.dataset)\n",
    "with open(os.path.join(data_path, 'config.txt'), 'r') as f:\n",
    "    data_config = eval(f.read())\n",
    "dataset = PretrainDataset(\n",
    "    data_path, data_config, args)\n",
    "# print(list(dataset))\n",
    "train_loader = DataLoader(dataset, batch_size=args.batch_size,\n",
    "                            collate_fn=PretrainDataset.collate_fn, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "with open('train_loader.pickle', 'wb') as file:\n",
    "    # Сериализация и сохранение объекта в файл\n",
    "    print('try')\n",
    "    pickle.dump(train_loader, file)\n",
    "\n",
    "trainer = Trainer(train_loader, None, args)\n",
    "if args.init_checkpoint is not None:\n",
    "    trainer.initialize_from_checkpoint(train=args.train)\n",
    "if args.train:\n",
    "    if args.save:\n",
    "        mol_out_dir = os.path.join(log_dir, 'mols')\n",
    "\n",
    "        if not os.path.exists(mol_out_dir):\n",
    "            os.makedirs(mol_out_dir)\n",
    "    else:\n",
    "        mol_out_dir = None\n",
    "    start = time()\n",
    "    trainer.fit(mol_out_dir=mol_out_dir)\n",
    "    print('Task model fitting done! Time {:.2f} seconds, Data: {}'.format(\n",
    "        time() - start, ctime()))\n",
    "\n",
    "elif args.resample:\n",
    "    trainer.resampling_molecules(resample_mode=0)\n",
    "else:\n",
    "    print('Start generating!')\n",
    "    start = time()\n",
    "    valid_ratio = []\n",
    "    unique_ratio = []\n",
    "    novel_ratio = []\n",
    "    valid_5atom_ratio = []\n",
    "    valid_39atom_ratio = []\n",
    "    for i in range(5):\n",
    "        _, Validity, Validity_without_check, Uniqueness, Novelty, _, mol_atom_size = trainer.generate_molecule(\n",
    "            args.gen_num)\n",
    "        valid_ratio.append(Validity)\n",
    "        unique_ratio.append(Uniqueness)\n",
    "        novel_ratio.append(Novelty)\n",
    "        valid_5atom_ratio.append(\n",
    "            np.sum(np.array(mol_atom_size) >= 5) / args.gen_num * 100)\n",
    "        valid_39atom_ratio.append(\n",
    "            np.sum(np.array(mol_atom_size) >= 39) / args.gen_num * 100)\n",
    "\n",
    "    print(\"validity: mean={:.2f}%, sd={:.2f}%, vals={}\".format(\n",
    "        np.mean(valid_ratio), np.std(valid_ratio), valid_ratio))\n",
    "    print(\"validity if atom >= 5: mean={:.2f}%, sd={:.2f}%, vals={}\".format(\n",
    "        np.mean(valid_5atom_ratio), np.std(valid_5atom_ratio), valid_5atom_ratio))\n",
    "    print(\"validity if atom >= 39: mean={:.2f}%, sd={:.2f}%, vals={}\".format(\n",
    "        np.mean(valid_39atom_ratio), np.std(valid_39atom_ratio), valid_39atom_ratio))\n",
    "    print(\"novelty: mean={:.2f}%, sd={:.2f}%, vals={}\".format(\n",
    "        np.mean(novel_ratio), np.std(novel_ratio), novel_ratio))\n",
    "    print(\"uniqueness: mean={:.2f}%, sd={:.2f}%, vals={}\".format(\n",
    "        np.mean(unique_ratio), np.std(unique_ratio), unique_ratio))\n",
    "    print('Task random generation done! Time {:.2f} seconds, Data: {}'.format(\n",
    "        time() - start, ctime()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0004, -0.0004,  0.0002]), tensor([2.0000, 2.0000, 1.9999]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_dist = torch.distributions.normal.Normal(torch.zeros([3]), 2*torch.ones([3]))\n",
    "z = prior_dist.sample((20000000,))\n",
    "z.mean(axis=0), z.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 1, 5, 5])\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n",
      "y.shape torch.Size([2, 2, 5, 5])\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bs.test_ZeroConv2d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
