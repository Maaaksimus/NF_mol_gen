{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa0547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from copy import deepcopy\n",
    "from dataloader import PretrainDataset\n",
    "from models.MolHF import MolHF\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool\n",
    "from distutils.util import strtobool\n",
    "from time import time, ctime\n",
    "import optimize_property as op\n",
    "from envs import environment as env\n",
    "from envs.timereport import TimeReport\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import set_random_seed\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from envs.environment import qed\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description='OptiModel')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_dir\", type=str, default='../save_optimization')\n",
    "    parser.add_argument('--dataset', type=str, default='zinc250k', choices=['zinc1500k', 'zinc250k'],\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--device', type=str, default='cuda')\n",
    "    parser.add_argument('--seed', type=int, default=23, help='random seed')\n",
    "    parser.add_argument(\"--property_model_path\", type=str, default=None)\n",
    "    parser.add_argument('--split', type=str, default=\"moflow\",\n",
    "                        help='choose the split type')\n",
    "    parser.add_argument('--is_test_idx', action='store_true', default=False, \n",
    "                        help='whether use test_idx')\n",
    "    parser.add_argument('--num_data', type=int,\n",
    "                        default=None, help='num of data to train')\n",
    "    \n",
    "    parser.add_argument('--num_workers', type=int, default=10,\n",
    "                        help='num works to generate data.')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "    parser.add_argument('--order', type=str, default='bfs',\n",
    "                        help='order of atom')\n",
    "    \n",
    "    # ******model args******\n",
    "    parser.add_argument('--deq_type', type=str,\n",
    "                        default='random', help='dequantization methods.')\n",
    "    parser.add_argument('--deq_scale', type=float, default=0.6,\n",
    "                        help='dequantization scale.(only for deq_type random)')\n",
    "    parser.add_argument('--n_block', type=int, default=4,\n",
    "                        help='num block')\n",
    "    parser.add_argument('--condition', action='store_false', default=True,\n",
    "                        help='latent variables on condition')\n",
    "    parser.add_argument('--moduls', type=str, default='Gen,DS', help='list of moduls to train')\n",
    "    \n",
    "    # ***atom model***\n",
    "    parser.add_argument('--a_num_flows', type=int, default=6,\n",
    "                        help='num of flows in RGBlock')\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='num of R-GCN layer in GraphAffineCoupling')\n",
    "    parser.add_argument('--hid_dim', type=int, default=256,\n",
    "                        help='hidden dim of R-GCN layer')\n",
    "    parser.add_argument('--st_type', type=str, default='sigmoid',\n",
    "                        help='architecture of st net, choice: [exp, sigmoid]')\n",
    "    parser.add_argument('--inv_rotate', action='store_false',\n",
    "                        default=True, help='whether rotate node feature')\n",
    "    # ***bond model***\n",
    "    parser.add_argument('--b_num_flows', type=int, default=3,\n",
    "                        help='num of flows in bond model')\n",
    "    parser.add_argument('--filter_size', type=int, default=256,\n",
    "                        help='num of filter size in AffineCoupling')\n",
    "    parser.add_argument('--inv_conv', action='store_false',\n",
    "                        default=True, help='whether use 1*1 conv')\n",
    "    parser.add_argument('--squeeze_fold', type=int, default=2,\n",
    "                        help='squeeze fold')\n",
    "    \n",
    "    parser.add_argument('--num_iter', type=int, default=200,\n",
    "                        help='num iter of optimization')\n",
    "    parser.add_argument('--learn_prior', action='store_false',\n",
    "                        default=True, help='learn log-var of gaussian prior.')\n",
    "    parser.add_argument('--init_checkpoint', type=str, default='../save_pretrain/zinc250k_model/checkpoint.pth',\n",
    "                    help='initialize from a checkpoint, if None, do not restore')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Base learning rate')\n",
    "    parser.add_argument('--opt_lr', type=float, default=0.001, help='optimization learning rate')\n",
    "    parser.add_argument('--lr_decay', type=float, default=1,\n",
    "                        help='Learning rate decay, applied every step of the optimization')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-5,\n",
    "                        help='L2 norm for the parameters')\n",
    "    parser.add_argument('--hidden', type=str, default=\"32\",\n",
    "                        help='Hidden dimension list for output regression')\n",
    "    parser.add_argument('--activation', type=str, default='tanh,tanh', help='Activations between layers')\n",
    "    parser.add_argument('--max_epochs', type=int, default=5, help='How many epochs to run in total?')\n",
    "\n",
    "    parser.add_argument('--temperature', type=float, default=0.6,\n",
    "                        help='temperature of the gaussian distributions')\n",
    "    parser.add_argument('--ratio', type=str, default='1,10', help='coefficients in loss')\n",
    "\n",
    "    parser.add_argument('--gen_num', type=int, default=100, help='Number of generated molecules')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "class PropNet(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=[128, 32], activ=[nn.Tanh(), nn.Tanh()]):\n",
    "        super(PropNet, self).__init__()\n",
    "\n",
    "        self.latent_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        vh = (self.latent_size,) + tuple(hidden_size) + (1,)\n",
    "        modules = []\n",
    "        for i in range(len(vh)-1):\n",
    "            modules.append(nn.Linear(vh[i], vh[i+1]))\n",
    "            if i < len(vh) - 2:\n",
    "                modules.append(activ[i])\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, h):\n",
    "        output = self.net(h)\n",
    "        return output\n",
    "\n",
    "\n",
    "class OptimModel(nn.Module):\n",
    "    def __init__(self, gen_model, hidden_size, activ):\n",
    "        super(OptimModel, self).__init__()\n",
    "        \n",
    "        self.model = gen_model\n",
    "\n",
    "        if isinstance(gen_model, type(VAE)):\n",
    "            self.latent_size = gen_model.q_mu.out_features\n",
    "        elif isinstance(gen_model, type(AAE)):\n",
    "            self.latent_size = gen_model.latent_size\n",
    "        else:\n",
    "            self.latent_node_length = gen_model.latent_node_length\n",
    "            self.latent_edge_length = gen_model.latent_edge_length\n",
    "            self.latent_size = self.latent_node_length + self.latent_edge_length\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.multi_model = PropNet(self.latent_size, hidden_size, activ)\n",
    "        \n",
    "\n",
    "    def encode(self, *input):\n",
    "        if isinstance(self.model, type(VAE)):\n",
    "            z = self.model.forward_encoder(input)\n",
    "        elif isinstance(self.model, type(AAE)):\n",
    "            z = self.model.encoder_forward(input)\n",
    "        else:\n",
    "            z, _, _  = self.model(input[0], input[1])  # z = [h, adj_h]\n",
    "            z = self.model.to_latent_format(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        z = self.encode(*input)\n",
    "        out = self.multi_model(z)\n",
    "        return out\n",
    "\n",
    "    def reverse(self, z):\n",
    "        out = self.model.to_molecule_format(z)\n",
    "        x, adj = self.model.reverse(out, true_adj=None)\n",
    "        return x, adj\n",
    "    \n",
    "\n",
    "def train_model(opt_model, optimizer, train_loader, metrics, tr, epoch, lrn_set=['DS']):\n",
    "    '''\n",
    "    Р”РµР»Р°РµС‚ РїСЂРѕС…РѕРґ РїРѕ РѕРґРЅРѕР№ СЌРїРѕС…Рµ СЃ С€Р°РіРѕРј РѕРїС‚РёРјРёР·Р°С‚РѕСЂР°\n",
    "    '''\n",
    "    log_step = 20\n",
    "    train_iter_per_epoch = len(train_loader)\n",
    "    global GEN_RATIO, MPROP_RATIO, args\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    opt_model.train()\n",
    "\n",
    "    total_pd_y = []\n",
    "    total_true_y = []\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        x = batch['node'].to(args.device)   # (bs,9,5)\n",
    "        adj = batch['adj'].to(args.device)   # (bs,4,9, 9)\n",
    "        true_y = batch['property'][:,0].float().unsqueeze(1).to(args.device)\n",
    "        true_sa_y = batch['property'][:,1].float().unsqueeze(1).to(args.device)\n",
    "        true_td_y = batch['property'][:,2].float().unsqueeze(1).to(args.device)\n",
    "\n",
    "        # model and loss\n",
    "        optimizer.zero_grad()\n",
    "        y, sa_y, td_y = opt_model(x, adj)\n",
    "\n",
    "        total_pd_y.append(y)\n",
    "        total_true_y.append(true_y)\n",
    "        \n",
    "        if 'Gen' in lrn_set:\n",
    "            out_z, out_logdet, _ = opt_model.model(x, adj)\n",
    "            loss_node, loss_edge = opt_model.model.log_prob(out_z, out_logdet)\n",
    "            loss_gen = loss_node + loss_edge\n",
    "        else:\n",
    "            loss_gen = torch.tensor([0], requires_grad=False).to(args.device)\n",
    "\n",
    "        if 'MPROP' in lrn_set:\n",
    "            loss_ds = metrics(y, true_y)\n",
    "        else:\n",
    "            loss_ds = torch.tensor([0], requires_grad=False).to(args.device)\n",
    "\n",
    "\n",
    "        loss = loss_gen * GEN_RATIO + loss_ds * MPROP_RATIO\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr.update()\n",
    "        \n",
    "        # Print log info\n",
    "        if (i + 1) % log_step == 0:  # i % args.log_step == 0:\n",
    "            print('Epoch [{}/{}], Iter [{}/{}], loss: {:.5f}, loss_gen: {:.5f}, loss_prop: {:.5f}, {:.2f} sec/iter, {:.2f} iters/sec: '.\n",
    "                    format(epoch + 1, args.max_epochs, i + 1, train_iter_per_epoch,\n",
    "                            loss.item(), loss_gen.item(), loss_ds.item(),\n",
    "                            tr.get_avg_time_per_iter(), tr.get_avg_iter_per_sec()))\n",
    "\n",
    "            t_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "            t_true_y = torch.cat(total_true_y, dim=-1)\n",
    "            print('Current R^2 score: ', r2_score(t_true_y.cpu().detach().numpy(), t_pd_y.cpu().detach().numpy()))\n",
    "\n",
    "            tr.print_summary()\n",
    "    \n",
    "    total_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "    total_true_y = torch.cat(total_true_y, dim=-1)\n",
    "    \n",
    "    mse = metrics(total_pd_y, total_true_y)\n",
    "    mae = mean_absolute_error(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "    r2 = r2_score(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "\n",
    "    print(\"Training, loss_mle:{}, loss_prop:{}, mse:{}, mae:{}, r2:{}\".format(loss_gen.item(), loss_ds.item(), mse, mae, r2))\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, metrics, col, tr, epoch):\n",
    "    log_step = 20\n",
    "    valid_iter_per_epoch = len(valid_loader)\n",
    "    \n",
    "    print(\"Validating...\")    \n",
    "    model.eval()\n",
    "    total_pd_y = []\n",
    "    total_true_y = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "\n",
    "            x = batch['node'].to(args.device)   # (bs,9,5)\n",
    "            adj = batch['adj'].to(args.device)   # (bs,4,9, 9)\n",
    "            true_y = batch['property'][:, col].unsqueeze(1).float().to(args.device)\n",
    "            # model and loss\n",
    "            y, _, _ = model(x, adj)\n",
    "            total_pd_y.append(y)\n",
    "            total_true_y.append(true_y)\n",
    "            loss_prop = metrics(y, true_y)\n",
    "            tr.update()\n",
    "            # Print log info\n",
    "            if (i + 1) % log_step == 0:  # i % args.log_step == 0:\n",
    "                print('Epoch [{}/{}], Iter [{}/{}], loss_prop: {:.5f}, {:.2f} sec/iter, {:.2f} iters/sec: '.\n",
    "                        format(epoch + 1, args.max_epochs, i + 1, valid_iter_per_epoch,\n",
    "                                loss_prop.item(),\n",
    "                                tr.get_avg_time_per_iter(), tr.get_avg_iter_per_sec()))\n",
    "                tr.print_summary()\n",
    "        total_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "        total_true_y = torch.cat(total_true_y, dim=-1)\n",
    "        mse = metrics(total_pd_y, total_true_y)\n",
    "        mae = mean_absolute_error(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "        r2 = r2_score(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "\n",
    "        print(\"Validating, loss_prop:{}, mse:{}, mae:{}, r2:{}\".format(loss_prop.item(), mse, mae, r2))\n",
    "        \n",
    "    return r2   \n",
    "\n",
    "\n",
    "def fit_model(opt_model, train_loader, val_loader, args, property_model_path, lrn_set=['DS']):\n",
    "    start = time()\n",
    "    print(\"Start at Time: {}\".format(ctime()))\n",
    "    print('Moduls for learning: ', lrn_set)\n",
    "\n",
    "    with open(f'r2_tr_{lrn_set}.txt', 'w') as f:\n",
    "        pass\n",
    "    with open('r2_val_mlp.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    metrics = nn.MSELoss()\n",
    "    best_metrics = float('-inf')\n",
    "    optimizer = torch.optim.Adam(opt_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    train_iter_per_epoch = len(train_loader)\n",
    "    valid_iter_per_epoch = len(val_loader)\n",
    "    tr = TimeReport(total_iter = args.max_epochs * (train_iter_per_epoch+valid_iter_per_epoch))\n",
    "\n",
    "    moduls_dict = {'Gen': opt_model.model, 'DS': opt_model.ds_model, 'SA': opt_model.sa_model, 'TD': opt_model.td_model}\n",
    "\n",
    "    for modul in set(moduls_dict.keys()).difference(lrn_set):\n",
    "        for param in moduls_dict[modul].parameters():\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "    for epoch in range(args.max_epochs):\n",
    "        print(\"In epoch {}, Time: {}\".format(epoch + 1, ctime()))\n",
    "        # op.generate_molecule(model, train_loader, args, epoch) # РїСЂРѕРІРµСЂРєР° С‚РµРєСѓС‰РµРіРѕ РєР°С‡РµСЃС‚РІР° РіРµРЅРµСЂР°С†РёРё СЃ РїСЂРёРЅС‚Р°РјРё РІР°Р»РёРґРЅРѕСЃС‚Рё Рё С‚.Рґ. \n",
    "        \n",
    "        train_model(opt_model, optimizer, train_loader, metrics, tr, epoch, lrn_set)\n",
    "        cur_metrics = validate_model(opt_model, valid_loader, metrics, 0, tr, epoch)\n",
    "        \n",
    "        if best_metrics < cur_metrics:\n",
    "            best_metrics = cur_metrics\n",
    "            print(\"Epoch {}, saving {} regression model to: {}\".format(epoch+1, args.hidden, property_model_path))\n",
    "            torch.save(opt_model.state_dict(), property_model_path)\n",
    "        \n",
    "    tr.print_summary()\n",
    "    tr.end()\n",
    "    \n",
    "    print(\"The model's training is done. Start at {}, End at {}, Total {:.2f}\".\n",
    "          format(ctime(start), ctime(), time()-start))\n",
    "    return opt_model\n",
    "\n",
    "\n",
    "def load_property_csv(filename, normilize=True):\n",
    "\n",
    "    df = pd.read_csv(filename)  # smiles, DS, SA, TD\n",
    "\n",
    "    min_max = lambda prop: (df[prop] - df[prop].min()) / (df[prop].max() - df[prop].min())\n",
    "    gauss = lambda prop: (df[prop] - df[prop].mean()) / df[prop].std()\n",
    "\n",
    "    if normilize:\n",
    "        df['DS'] = df['DS'].clip(-12,-5)\n",
    "        df['DS'] = min_max('DS')\n",
    "        \n",
    "        # df['SA'] = df['SA'].clip(-12, -5)\n",
    "        df['SA'] = min_max('SA')\n",
    "        \n",
    "        df['TD'] = 1 - min_max('TD')\n",
    "        \n",
    "    tuples = [tuple((x[1]*x[2]*x[3])**(1/3)) for x in df.values]\n",
    "\n",
    "    print('Load {} done, length: {}'.format(filename, len(tuples)))\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "print(\"Start at Time: {}\".format(ctime()))\n",
    "args = torch.load('MolHF_conf.json')\n",
    "# set_random_seed(args.seed)\n",
    "# configuration\n",
    "num2atom = {0: 6, 1: 7, 2: 8, 3: 9, 4: 15, 5: 16, 6: 17, 7: 35, 8: 53}\n",
    "atom_valency = {6: 4, 7: 3, 8: 2, 9: 1, 15: 3, 16: 2, 17: 1, 35: 1, 53: 1}\n",
    "\n",
    "data_path = os.path.join('../data_preprocessed', args.dataset)\n",
    "with open(os.path.join(data_path, 'config.txt'), 'r') as f:\n",
    "    data_config = eval(f.read())\n",
    "\n",
    "with open(\"../dataset/zinc250k/moflow_idx.json\", \"r\") as f:\n",
    "    train_idx, valid_idx = json.load(f)\n",
    "dataset = PretrainDataset(\"../data_preprocessed/{}\".format(args.dataset), data_config, args)\n",
    "train_dataset = deepcopy(dataset)\n",
    "train_dataset._indices = train_idx # РґР°РЅРЅС‹Рµ С…СЂР°РЅСЏС‚СЃСЏ РІСЃРµ, РЅРѕ Р±РµСЂСѓС‚СЃСЏ С‚РѕР»СЊРєРѕ С‚Рµ, РєРѕС‚РѕСЂС‹Рµ РµСЃС‚СЊ РІ СЃРїРёСЃРєРµ РёРЅРґРµРєСЃРѕРІ\n",
    "valid_dataset = deepcopy(dataset)\n",
    "valid_dataset._indices = valid_idx # Р°РЅР°Р»РѕРіРёС‡РЅРѕ\n",
    "\n",
    "if args.hidden in ('', ','):\n",
    "    hidden = []\n",
    "else:\n",
    "    hidden = [int(d) for d in args.hidden.strip(',').split(',')]\n",
    "print('Hidden dim for output regression: ', hidden)\n",
    "\n",
    "if args.ratio in ('',','):\n",
    "    ratio = []\n",
    "else:\n",
    "    GEN_RATIO, MPROP_RATIO = [float(d) for d in args.ratio.strip(',').split(',')]\n",
    "\n",
    "if args.moduls in ('',','):\n",
    "    raise ValueError('empty moduls list')\n",
    "else:\n",
    "    moduls_list = [mod for mod in args.moduls.strip(',').split(',')]\n",
    "\n",
    "if args.moduls in ('',','):\n",
    "    raise ValueError('empty activation list')\n",
    "else:\n",
    "    acti_dict = {'tanh': nn.Tanh(), 'sigm': nn.Sigmoid(), 'relu': nn.ReLU()}\n",
    "    activ = [acti_dict[acti] for acti in args.activation.strip(',').split(',')]\n",
    "\n",
    "if args.property_model_path is None:\n",
    "    print('in')\n",
    "    mol_property = load_property_csv('../docking/DS_data/qed_dataset_all.csv')\n",
    "\n",
    "    train_dataset.is_mol_property = True\n",
    "    train_dataset.mol_property = mol_property\n",
    "    valid_dataset.is_mol_property = True\n",
    "    valid_dataset.mol_property = mol_property\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size,collate_fn=PretrainDataset.collate_fn, num_workers=args.num_workers, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size,collate_fn=PretrainDataset.collate_fn, num_workers=args.num_workers, drop_last=True)\n",
    "    \n",
    "    property_model_path = os.path.join(args.model_dir, '{}_{}-{}-{}-{}_{}.pth'.format(args.hidden, GEN_RATIO, DS_RATIO, SA_RATIO, TD_RATIO, moduls_list))\n",
    "    \n",
    "    gen_model = MolHF(data_config, args).to(args.device)\n",
    "    op.initialize_from_checkpoint(gen_model, args)\n",
    "\n",
    "    opti_model = OptimModel(gen_model, hidden, activ).to(args.device)\n",
    "    property_model = fit_model(opti_model, train_loader, valid_loader, args, property_model_path, moduls_list)   \n",
    "# else:\n",
    "#     prop_list = load_property_csv('tmp_docking_dataset.csv')\n",
    "#     train_prop = [prop_list[i] for i in train_idx]\n",
    "\n",
    "#     # DMNP\n",
    "#     dmnp_smiles = 'CC1CCC(C2=C1C=CC(=C2)C)C(C)CCC(=O)O'\n",
    "#     train_prop = [tuple(op.get_mol_property(dmnp_smiles) + [dmnp_smiles])]\n",
    "    \n",
    "#     test_prop = [prop_list[i] for i in valid_idx]\n",
    "#     property_model_path = os.path.join(args.model_dir, args.property_model_path)\n",
    "    \n",
    "#     model = MolHF(data_config, args).to(args.device)\n",
    "#     op.initialize_from_checkpoint(model, args)\n",
    "    \n",
    "#     property_model = OptimModel(model, hidden).to(args.device)\n",
    "#     property_model.load_state_dict(torch.load(property_model_path, map_location=args.device))\n",
    "\n",
    "#     property_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moses.vae import VAE\n",
    "from moses.script_utils import read_smiles_csv\n",
    "\n",
    "def fit(mlp):\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        # print('Epoch: ', epoch + 1)\n",
    "\n",
    "        # current_loss = 0.\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = mlp(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_true.append(targets)\n",
    "            y_pred.append(outputs.ravel())\n",
    "\n",
    "            # current_loss += loss.item()\n",
    "\n",
    "            # if i % 500 == 499:\n",
    "            #   print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n",
    "            #   current_loss = 0.0\n",
    "\n",
    "    total_pred = torch.cat(y_pred, dim=-1)\n",
    "    total_true = torch.cat(y_true, dim=-1)\n",
    "    r2 = r2_score(total_true.detach().cpu(), total_pred.detach().cpu())\n",
    "    print('R^2 score: ', r2)\n",
    "\n",
    "    return r2\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'input_size': [128],\n",
    "    'hidden_size': [[64], [32], [16], [8]],\n",
    "    'activ': [[nn.Tanh(), nn.Tanh()], [nn.ReLU(), nn.Tanh()], [nn.ReLU(), nn.Sigmoid()]]\n",
    "}\n",
    "\n",
    "dataset = pd.read_csv('./data_preprocessed/zinc250k/zinc250k_property.csv')\n",
    "\n",
    "vocab = torch.load('./my_scripts/vae/vocabz.json')\n",
    "config = torch.load('./my_scripts/vae/configz.json')\n",
    "\n",
    "tvae = VAE(vocab, config).to('cuda')\n",
    "tvae.load_state_dict(torch.load('./my_scripts/vae/vaez.pt'))\n",
    "\n",
    "def collate(data):\n",
    "    data.sort(key=len, reverse=True)\n",
    "    tensors = [tvae.string2tensor(string, device='cuda')\n",
    "                for string in data]\n",
    "\n",
    "    return tensors\n",
    "\n",
    "tr_data = dataset['smile'].values.tolist()\n",
    "coll_tr_data = collate(tr_data)\n",
    "y = dataset['qed'].values.tolist()\n",
    "\n",
    "latent_vectors = []\n",
    "\n",
    "for i, el in enumerate(tqdm((coll_tr_data))):\n",
    "    latent_vectors.append(tvae.forward_encoder([el])[0].detach())\n",
    "\n",
    "trn_z, trn_y, tst_z, tst_y = train_test_split(latent_vectors, y, train_size=0.8, random_state=42)\n",
    "\n",
    "dataset = TensorDataset(trn_z, trn_y)\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "best_r2 = -100\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    print('Fitting for parametrs: ', param)\n",
    "    mlp = PropNet(**param).to('cuda')\n",
    "    curr_r2 = fit(mlp)\n",
    "    if curr_r2 > best_r2:\n",
    "        best_par = param\n",
    "        best_r2 = curr_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30375029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvae.eval()\n",
    "tvae.forward_encoder(coll_tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13665b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for parametrs:  {'activ': [Tanh(), Tanh()], 'hidden_size': [64], 'input_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:25<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  -0.0021789734557595697\n",
      "Fitting for parametrs:  {'activ': [Tanh(), Tanh()], 'hidden_size': [32], 'input_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:25<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  -0.0012928297620233842\n",
      "Fitting for parametrs:  {'activ': [Tanh(), Tanh()], 'hidden_size': [16], 'input_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:24<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  -0.0010401864948852424\n",
      "Fitting for parametrs:  {'activ': [Tanh(), Tanh()], 'hidden_size': [8], 'input_size': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting for parametrs: \u001b[39m\u001b[38;5;124m'\u001b[39m, param)\n\u001b[0;32m     10\u001b[0m mlp \u001b[38;5;241m=\u001b[39m PropNet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m curr_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_r2 \u001b[38;5;241m>\u001b[39m best_r2:\n\u001b[0;32m     13\u001b[0m     best_par \u001b[38;5;241m=\u001b[39m param\n",
      "Cell \u001b[1;32mIn[59], line 29\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(mlp)\u001b[0m\n\u001b[0;32m     26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m         y_true\u001b[38;5;241m.\u001b[39mappend(targets)\n\u001b[1;32m---> 29\u001b[0m         y_pred\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# current_loss += loss.item()\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# if i % 500 == 499:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m#   print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m#   current_loss = 0.0\u001b[39;00m\n\u001b[0;32m     37\u001b[0m total_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trn_z, tst_z, trn_y, tst_y = train_test_split(latent_vectors, y, test_size=0.1, random_state=42)\n",
    "\n",
    "dataset = TensorDataset(torch.concat(trn_z), torch.tensor(trn_y))\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "best_r2 = -100\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    print('Fitting for parametrs: ', param)\n",
    "    mlp = PropNet(**param).to('cuda')\n",
    "    curr_r2 = fit(mlp)\n",
    "    if curr_r2 > best_r2:\n",
    "        best_par = param\n",
    "        best_r2 = curr_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c680673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([0.3219, 0.8781, 0.6944, 0.3095, 0.6282, 0.0828, 0.2982, 0.8496, 0.1932,\n",
       "          0.6736]),\n",
       "  tensor([3.1247e-02, 9.0621e-01, 7.4620e-01, 4.4423e-04, 7.3117e-01, 4.5465e-01,\n",
       "          4.4921e-01, 2.9378e-01, 2.2737e-01, 2.0457e-01]),\n",
       "  tensor([0.5903, 0.8211, 0.9442, 0.1659, 0.9482, 0.9503, 0.1886, 0.7294, 0.1079,\n",
       "          0.9284]),\n",
       "  tensor([0.7521, 0.2587, 0.3019, 0.7025, 0.3229, 0.2826, 0.5517, 0.7015, 0.3845,\n",
       "          0.8279]),\n",
       "  tensor([0.6104, 0.2007, 0.7324, 0.6335, 0.9158, 0.7295, 0.7848, 0.5423, 0.4331,\n",
       "          0.2946]),\n",
       "  tensor([0.5637, 0.1099, 0.3489, 0.3438, 0.7324, 0.6600, 0.3004, 0.8696, 0.6557,\n",
       "          0.3986]),\n",
       "  tensor([0.5537, 0.4331, 0.6326, 0.3550, 0.2853, 0.7788, 0.7651, 0.5271, 0.6289,\n",
       "          0.7622]),\n",
       "  tensor([0.5428, 0.3887, 0.5909, 0.1478, 0.1973, 0.8802, 0.0861, 0.7267, 0.8672,\n",
       "          0.9376])],\n",
       " [tensor([0.0318, 0.9820, 0.9198, 0.4012, 0.4799, 0.0123, 0.9151, 0.3717, 0.0895,\n",
       "          0.1687]),\n",
       "  tensor([0.7162, 0.2041, 0.8000, 0.7285, 0.8809, 0.4470, 0.4695, 0.7822, 0.0072,\n",
       "          0.0751])]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split([torch.rand(10) for _ in range(10)], train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114d6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([128, 128])\n",
      "Batch y: torch.Size([128])\n",
      "Epoch:  1\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -4.063088515274099\n",
      "Epoch:  2\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.7967970075185424\n",
      "Epoch:  3\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.47570984980981734\n",
      "Epoch:  4\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.26256089663954985\n",
      "Epoch:  5\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.13744932673079924\n",
      "Epoch:  6\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.07039101780657253\n",
      "Epoch:  7\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.03653552307181851\n",
      "Epoch:  8\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.019201389894608845\n",
      "Epoch:  9\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.011001967905299281\n",
      "Epoch:  10\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.006183857844625162\n",
      "Epoch:  11\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.003871358856215812\n",
      "Epoch:  12\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.002391801099371982\n",
      "Epoch:  13\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0017072726800433369\n",
      "Epoch:  14\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0013850394669023824\n",
      "Epoch:  15\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0009972051933351267\n",
      "Epoch:  16\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0007533704348110515\n",
      "Epoch:  17\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0009715822165656807\n",
      "Epoch:  18\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.000755355589995288\n",
      "Epoch:  19\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0008259408780071364\n",
      "Epoch:  20\n",
      "torch.Size([224384]) torch.Size([224384])\n",
      "R^2 score:  -0.0007589302051700386\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "z_vae = np.load('./my_scripts/vae/zvae.npy', allow_pickle=True)\n",
    "\n",
    "dataset = TensorDataset(torch.from_numpy(z_vae).float(), torch.tensor(moses_tab['QED'].values).float())\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "for batch_x, batch_y in train_loader:\n",
    "    print(\"Batch x:\", batch_x.shape)\n",
    "    print(\"Batch y:\", batch_y.shape)\n",
    "    break\n",
    "\n",
    "class PropNet(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=[128, 32], activ=[nn.Tanh(), nn.Tanh()]):\n",
    "        super(PropNet, self).__init__()\n",
    "\n",
    "        self.latent_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        vh = (self.latent_size,) + tuple(hidden_size) + (1,)\n",
    "        modules = []\n",
    "        for i in range(len(vh)-1):\n",
    "            modules.append(nn.Linear(vh[i], vh[i+1]))\n",
    "            if i < len(vh) - 2:\n",
    "                modules.append(activ[i])\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, h):\n",
    "        output = self.net(h)\n",
    "        return output\n",
    "    \n",
    "\n",
    "mlp = PropNet(128, [32], [nn.ReLU(), nn.Tanh()]).to('cuda')\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "def fit(mlp):\n",
    "    for epoch in range(20):\n",
    "        print('Epoch: ', epoch + 1)\n",
    "\n",
    "        # current_loss = 0.\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = mlp(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_true.append(targets)\n",
    "            y_pred.append(outputs.ravel())\n",
    "\n",
    "            # current_loss += loss.item()\n",
    "\n",
    "            # if i % 500 == 499:\n",
    "            #   print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n",
    "            #   current_loss = 0.0\n",
    "\n",
    "        total_pred = torch.cat(y_pred, dim=-1)\n",
    "        total_true = torch.cat(y_true, dim=-1)\n",
    "        print(total_true.shape, total_pred.shape)\n",
    "        r2 = r2_score(total_true.detach().cpu(), total_pred.detach().cpu())\n",
    "        print('R^2 score: ', r2)\n",
    "\n",
    "        return r2\n",
    "\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d620001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76349195, 0.8065782 , 0.89652872, ..., 0.82943178, 0.75810816,\n",
       "       0.75177321])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses_tab['QED'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e62b8714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>QED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=Cc1cccc(-c2cccs2)c1O</td>\n",
       "      <td>0.763492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cc1cc2c(=O)n(C[C@@H](O)c3ccc(F)c(F)c3)cnc2s1</td>\n",
       "      <td>0.806578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCc1nn(C)cc1C(=O)N1CCCC[C@H]1CNC(=O)c1ccccc1</td>\n",
       "      <td>0.896529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[C@H](NC(=O)c1cc(Cl)c[nH]1)c1nnc2ccccn12</td>\n",
       "      <td>0.776011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(C)Nc1ncccc1C(=O)N1CCN(C2=[NH+]C[C@H](C)S2)CC1</td>\n",
       "      <td>0.818938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224504</th>\n",
       "      <td>C[C@@H]1Oc2ccccc2O[C@H]1C(=O)Nc1ccc(NS(=O)(=O)...</td>\n",
       "      <td>0.627099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224505</th>\n",
       "      <td>Cc1ccccc1[C@@](C)([NH3+])[C@H]1CCOC2(CCOCC2)C1</td>\n",
       "      <td>0.909028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224506</th>\n",
       "      <td>CN(C)c1ncccc1N(C)S(=O)(=O)/C=C/c1ccccc1Cl</td>\n",
       "      <td>0.829432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224507</th>\n",
       "      <td>Cc1cccc(-n2ccnc2S[C@@H](C)C(=O)N(C)CC(=O)NC2CC...</td>\n",
       "      <td>0.758108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224508</th>\n",
       "      <td>CC(=O)Nc1ccc([S@](=O)Cc2nc3ccccc3n2C(F)F)cc1</td>\n",
       "      <td>0.751773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224509 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SMILES       QED\n",
       "0                                  O=Cc1cccc(-c2cccs2)c1O  0.763492\n",
       "1            Cc1cc2c(=O)n(C[C@@H](O)c3ccc(F)c(F)c3)cnc2s1  0.806578\n",
       "2            CCc1nn(C)cc1C(=O)N1CCCC[C@H]1CNC(=O)c1ccccc1  0.896529\n",
       "3               C[C@H](NC(=O)c1cc(Cl)c[nH]1)c1nnc2ccccn12  0.776011\n",
       "4        CC(C)Nc1ncccc1C(=O)N1CCN(C2=[NH+]C[C@H](C)S2)CC1  0.818938\n",
       "...                                                   ...       ...\n",
       "224504  C[C@@H]1Oc2ccccc2O[C@H]1C(=O)Nc1ccc(NS(=O)(=O)...  0.627099\n",
       "224505     Cc1ccccc1[C@@](C)([NH3+])[C@H]1CCOC2(CCOCC2)C1  0.909028\n",
       "224506          CN(C)c1ncccc1N(C)S(=O)(=O)/C=C/c1ccccc1Cl  0.829432\n",
       "224507  Cc1cccc(-n2ccnc2S[C@@H](C)C(=O)N(C)CC(=O)NC2CC...  0.758108\n",
       "224508       CC(=O)Nc1ccc([S@](=O)Cc2nc3ccccc3n2C(F)F)cc1  0.751773\n",
       "\n",
       "[224509 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses_tab = pd.read_csv('./my_scripts/moses_train.csv')\n",
    "moses_tab['QED'] = moses_tab['SMILES'].apply(lambda x: qed(Chem.MolFromSmiles(x)))\n",
    "moses_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870dfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ml39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
