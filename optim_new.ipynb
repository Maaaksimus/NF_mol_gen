{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from copy import deepcopy\n",
    "from dataloader import PretrainDataset\n",
    "from models.MolHF import MolHF\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import Pool\n",
    "from distutils.util import strtobool\n",
    "from time import time, ctime\n",
    "import optimize_property as op\n",
    "from envs import environment as env\n",
    "from envs.timereport import TimeReport\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import set_random_seed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description='OptiModel')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_dir\", type=str, default='../save_optimization')\n",
    "    parser.add_argument('--dataset', type=str, default='zinc250k', choices=['zinc1500k', 'zinc250k'],\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--device', type=str, default='cuda')\n",
    "    parser.add_argument('--seed', type=int, default=23, help='random seed')\n",
    "    parser.add_argument(\"--property_model_path\", type=str, default=None)\n",
    "    parser.add_argument('--split', type=str, default=\"moflow\",\n",
    "                        help='choose the split type')\n",
    "    parser.add_argument('--is_test_idx', action='store_true', default=False, \n",
    "                        help='whether use test_idx')\n",
    "    parser.add_argument('--num_data', type=int,\n",
    "                        default=None, help='num of data to train')\n",
    "    \n",
    "    parser.add_argument('--num_workers', type=int, default=10,\n",
    "                        help='num works to generate data.')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "    parser.add_argument('--order', type=str, default='bfs',\n",
    "                        help='order of atom')\n",
    "    \n",
    "    # ******model args******\n",
    "    parser.add_argument('--deq_type', type=str,\n",
    "                        default='random', help='dequantization methods.')\n",
    "    parser.add_argument('--deq_scale', type=float, default=0.6,\n",
    "                        help='dequantization scale.(only for deq_type random)')\n",
    "    parser.add_argument('--n_block', type=int, default=4,\n",
    "                        help='num block')\n",
    "    parser.add_argument('--condition', action='store_false', default=True,\n",
    "                        help='latent variables on condition')\n",
    "    parser.add_argument('--moduls', type=str, default='Gen,DS', help='list of moduls to train')\n",
    "    \n",
    "    # ***atom model***\n",
    "    parser.add_argument('--a_num_flows', type=int, default=6,\n",
    "                        help='num of flows in RGBlock')\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='num of R-GCN layer in GraphAffineCoupling')\n",
    "    parser.add_argument('--hid_dim', type=int, default=256,\n",
    "                        help='hidden dim of R-GCN layer')\n",
    "    parser.add_argument('--st_type', type=str, default='sigmoid',\n",
    "                        help='architecture of st net, choice: [exp, sigmoid]')\n",
    "    parser.add_argument('--inv_rotate', action='store_false',\n",
    "                        default=True, help='whether rotate node feature')\n",
    "    # ***bond model***\n",
    "    parser.add_argument('--b_num_flows', type=int, default=3,\n",
    "                        help='num of flows in bond model')\n",
    "    parser.add_argument('--filter_size', type=int, default=256,\n",
    "                        help='num of filter size in AffineCoupling')\n",
    "    parser.add_argument('--inv_conv', action='store_false',\n",
    "                        default=True, help='whether use 1*1 conv')\n",
    "    parser.add_argument('--squeeze_fold', type=int, default=2,\n",
    "                        help='squeeze fold')\n",
    "    \n",
    "    parser.add_argument('--num_iter', type=int, default=200,\n",
    "                        help='num iter of optimization')\n",
    "    parser.add_argument('--learn_prior', action='store_false',\n",
    "                        default=True, help='learn log-var of gaussian prior.')\n",
    "    parser.add_argument('--init_checkpoint', type=str, default='../save_pretrain/zinc250k_model/checkpoint.pth',\n",
    "                    help='initialize from a checkpoint, if None, do not restore')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Base learning rate')\n",
    "    parser.add_argument('--opt_lr', type=float, default=0.001, help='optimization learning rate')\n",
    "    parser.add_argument('--lr_decay', type=float, default=1,\n",
    "                        help='Learning rate decay, applied every step of the optimization')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-5,\n",
    "                        help='L2 norm for the parameters')\n",
    "    parser.add_argument('--hidden', type=str, default=\"32\",\n",
    "                        help='Hidden dimension list for output regression')\n",
    "    parser.add_argument('--activation', type=str, default='tanh,tanh', help='Activations between layers')\n",
    "    parser.add_argument('--max_epochs', type=int, default=5, help='How many epochs to run in total?')\n",
    "\n",
    "    parser.add_argument('--temperature', type=float, default=0.6,\n",
    "                        help='temperature of the gaussian distributions')\n",
    "    parser.add_argument('--ratio', type=str, default='1,10', help='coefficients in loss')\n",
    "\n",
    "    parser.add_argument('--gen_num', type=int, default=100, help='Number of generated molecules')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "class PropNet(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=[128, 32], activ=[nn.Tanh(), nn.Tanh()]):\n",
    "        super(PropNet, self).__init__()\n",
    "\n",
    "        self.latent_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        vh = (self.latent_size,) + tuple(hidden_size) + (1,)\n",
    "        modules = []\n",
    "        for i in range(len(vh)-1):\n",
    "            modules.append(nn.Linear(vh[i], vh[i+1]))\n",
    "            if i < len(vh) - 2:\n",
    "                modules.append(activ[i])\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, h):\n",
    "        output = self.net(h)\n",
    "        return output\n",
    "\n",
    "\n",
    "class OptimModel(nn.Module):\n",
    "    def __init__(self, gen_model, hidden_size, activ):\n",
    "        super(OptimModel, self).__init__()\n",
    "        \n",
    "        self.model = gen_model\n",
    "\n",
    "        if isinstance(gen_model, type(VAE)):\n",
    "            self.latent_size = gen_model.q_mu.out_features\n",
    "        elif isinstance(gen_model, type(AAE)):\n",
    "            self.latent_size = gen_model.latent_size\n",
    "        else:\n",
    "            self.latent_node_length = gen_model.latent_node_length\n",
    "            self.latent_edge_length = gen_model.latent_edge_length\n",
    "            self.latent_size = self.latent_node_length + self.latent_edge_length\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.multi_model = PropNet(self.latent_size, hidden_size, activ)\n",
    "        \n",
    "\n",
    "    def encode(self, *input):\n",
    "        if isinstance(self.model, type(VAE)):\n",
    "            z = self.model.forward_encoder(input)\n",
    "        elif isinstance(self.model, type(AAE)):\n",
    "            z = self.model.encoder_forward(input)\n",
    "        else:\n",
    "            z, _, _  = self.model(input[0], input[1])  # z = [h, adj_h]\n",
    "            z = self.model.to_latent_format(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        z = self.encode(*input)\n",
    "        out = self.multi_model(z)\n",
    "        return out\n",
    "\n",
    "    def reverse(self, z):\n",
    "        out = self.model.to_molecule_format(z)\n",
    "        x, adj = self.model.reverse(out, true_adj=None)\n",
    "        return x, adj\n",
    "    \n",
    "\n",
    "def train_model(opt_model, optimizer, train_loader, metrics, tr, epoch, lrn_set=['DS']):\n",
    "    '''\n",
    "    Р”РµР»Р°РµС‚ РїСЂРѕС…РѕРґ РїРѕ РѕРґРЅРѕР№ СЌРїРѕС…Рµ СЃ С€Р°РіРѕРј РѕРїС‚РёРјРёР·Р°С‚РѕСЂР°\n",
    "    '''\n",
    "    log_step = 20\n",
    "    train_iter_per_epoch = len(train_loader)\n",
    "    global GEN_RATIO, MPROP_RATIO, args\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    opt_model.train()\n",
    "\n",
    "    total_pd_y = []\n",
    "    total_true_y = []\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        x = batch['node'].to(args.device)   # (bs,9,5)\n",
    "        adj = batch['adj'].to(args.device)   # (bs,4,9, 9)\n",
    "        true_y = batch['property'][:,0].float().unsqueeze(1).to(args.device)\n",
    "        true_sa_y = batch['property'][:,1].float().unsqueeze(1).to(args.device)\n",
    "        true_td_y = batch['property'][:,2].float().unsqueeze(1).to(args.device)\n",
    "\n",
    "        # model and loss\n",
    "        optimizer.zero_grad()\n",
    "        y, sa_y, td_y = opt_model(x, adj)\n",
    "\n",
    "        total_pd_y.append(y)\n",
    "        total_true_y.append(true_y)\n",
    "        \n",
    "        if 'Gen' in lrn_set:\n",
    "            out_z, out_logdet, _ = opt_model.model(x, adj)\n",
    "            loss_node, loss_edge = opt_model.model.log_prob(out_z, out_logdet)\n",
    "            loss_gen = loss_node + loss_edge\n",
    "        else:\n",
    "            loss_gen = torch.tensor([0], requires_grad=False).to(args.device)\n",
    "\n",
    "        if 'MPROP' in lrn_set:\n",
    "            loss_ds = metrics(y, true_y)\n",
    "        else:\n",
    "            loss_ds = torch.tensor([0], requires_grad=False).to(args.device)\n",
    "\n",
    "\n",
    "        loss = loss_gen * GEN_RATIO + loss_ds * MPROP_RATIO\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr.update()\n",
    "        \n",
    "        # Print log info\n",
    "        if (i + 1) % log_step == 0:  # i % args.log_step == 0:\n",
    "            print('Epoch [{}/{}], Iter [{}/{}], loss: {:.5f}, loss_gen: {:.5f}, loss_prop: {:.5f}, {:.2f} sec/iter, {:.2f} iters/sec: '.\n",
    "                    format(epoch + 1, args.max_epochs, i + 1, train_iter_per_epoch,\n",
    "                            loss.item(), loss_gen.item(), loss_ds.item(),\n",
    "                            tr.get_avg_time_per_iter(), tr.get_avg_iter_per_sec()))\n",
    "\n",
    "            t_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "            t_true_y = torch.cat(total_true_y, dim=-1)\n",
    "            print('Current R^2 score: ', r2_score(t_true_y.cpu().detach().numpy(), t_pd_y.cpu().detach().numpy()))\n",
    "\n",
    "            tr.print_summary()\n",
    "    \n",
    "    total_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "    total_true_y = torch.cat(total_true_y, dim=-1)\n",
    "    \n",
    "    mse = metrics(total_pd_y, total_true_y)\n",
    "    mae = mean_absolute_error(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "    r2 = r2_score(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "\n",
    "    print(\"Training, loss_mle:{}, loss_prop:{}, mse:{}, mae:{}, r2:{}\".format(loss_gen.item(), loss_ds.item(), mse, mae, r2))\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, metrics, col, tr, epoch):\n",
    "    log_step = 20\n",
    "    valid_iter_per_epoch = len(valid_loader)\n",
    "    \n",
    "    print(\"Validating...\")    \n",
    "    model.eval()\n",
    "    total_pd_y = []\n",
    "    total_true_y = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "\n",
    "            x = batch['node'].to(args.device)   # (bs,9,5)\n",
    "            adj = batch['adj'].to(args.device)   # (bs,4,9, 9)\n",
    "            true_y = batch['property'][:, col].unsqueeze(1).float().to(args.device)\n",
    "            # model and loss\n",
    "            y, _, _ = model(x, adj)\n",
    "            total_pd_y.append(y)\n",
    "            total_true_y.append(true_y)\n",
    "            loss_prop = metrics(y, true_y)\n",
    "            tr.update()\n",
    "            # Print log info\n",
    "            if (i + 1) % log_step == 0:  # i % args.log_step == 0:\n",
    "                print('Epoch [{}/{}], Iter [{}/{}], loss_prop: {:.5f}, {:.2f} sec/iter, {:.2f} iters/sec: '.\n",
    "                        format(epoch + 1, args.max_epochs, i + 1, valid_iter_per_epoch,\n",
    "                                loss_prop.item(),\n",
    "                                tr.get_avg_time_per_iter(), tr.get_avg_iter_per_sec()))\n",
    "                tr.print_summary()\n",
    "        total_pd_y = torch.cat(total_pd_y, dim=-1)\n",
    "        total_true_y = torch.cat(total_true_y, dim=-1)\n",
    "        mse = metrics(total_pd_y, total_true_y)\n",
    "        mae = mean_absolute_error(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "        r2 = r2_score(total_true_y.cpu().detach().numpy(), total_pd_y.cpu().detach().numpy())\n",
    "\n",
    "        print(\"Validating, loss_prop:{}, mse:{}, mae:{}, r2:{}\".format(loss_prop.item(), mse, mae, r2))\n",
    "        \n",
    "    return r2   \n",
    "\n",
    "\n",
    "def fit_model(opt_model, train_loader, val_loader, args, property_model_path, lrn_set=['DS']):\n",
    "    start = time()\n",
    "    print(\"Start at Time: {}\".format(ctime()))\n",
    "    print('Moduls for learning: ', lrn_set)\n",
    "\n",
    "    with open(f'r2_tr_{lrn_set}.txt', 'w') as f:\n",
    "        pass\n",
    "    with open('r2_val_mlp.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    metrics = nn.MSELoss()\n",
    "    best_metrics = float('-inf')\n",
    "    optimizer = torch.optim.Adam(opt_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    train_iter_per_epoch = len(train_loader)\n",
    "    valid_iter_per_epoch = len(val_loader)\n",
    "    tr = TimeReport(total_iter = args.max_epochs * (train_iter_per_epoch+valid_iter_per_epoch))\n",
    "\n",
    "    moduls_dict = {'Gen': opt_model.model, 'DS': opt_model.ds_model, 'SA': opt_model.sa_model, 'TD': opt_model.td_model}\n",
    "\n",
    "    for modul in set(moduls_dict.keys()).difference(lrn_set):\n",
    "        for param in moduls_dict[modul].parameters():\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "    for epoch in range(args.max_epochs):\n",
    "        print(\"In epoch {}, Time: {}\".format(epoch + 1, ctime()))\n",
    "        # op.generate_molecule(model, train_loader, args, epoch) # РїСЂРѕРІРµСЂРєР° С‚РµРєСѓС‰РµРіРѕ РєР°С‡РµСЃС‚РІР° РіРµРЅРµСЂР°С†РёРё СЃ РїСЂРёРЅС‚Р°РјРё РІР°Р»РёРґРЅРѕСЃС‚Рё Рё С‚.Рґ. \n",
    "        \n",
    "        train_model(opt_model, optimizer, train_loader, metrics, tr, epoch, lrn_set)\n",
    "        cur_metrics = validate_model(opt_model, valid_loader, metrics, 0, tr, epoch)\n",
    "        \n",
    "        if best_metrics < cur_metrics:\n",
    "            best_metrics = cur_metrics\n",
    "            print(\"Epoch {}, saving {} regression model to: {}\".format(epoch+1, args.hidden, property_model_path))\n",
    "            torch.save(opt_model.state_dict(), property_model_path)\n",
    "        \n",
    "    tr.print_summary()\n",
    "    tr.end()\n",
    "    \n",
    "    print(\"The model's training is done. Start at {}, End at {}, Total {:.2f}\".\n",
    "          format(ctime(start), ctime(), time()-start))\n",
    "    return opt_model\n",
    "\n",
    "\n",
    "def load_property_csv(filename, normilize=True):\n",
    "\n",
    "    df = pd.read_csv(filename)  # smiles, DS, SA, TD\n",
    "\n",
    "    min_max = lambda prop: (df[prop] - df[prop].min()) / (df[prop].max() - df[prop].min())\n",
    "    gauss = lambda prop: (df[prop] - df[prop].mean()) / df[prop].std()\n",
    "\n",
    "    if normilize:\n",
    "        df['DS'] = df['DS'].clip(-12,-5)\n",
    "        df['DS'] = min_max('DS')\n",
    "        \n",
    "        # df['SA'] = df['SA'].clip(-12, -5)\n",
    "        df['SA'] = min_max('SA')\n",
    "        \n",
    "        df['TD'] = 1 - min_max('TD')\n",
    "        \n",
    "    tuples = [tuple((x[1]*x[2]*x[3])**(1/3)) for x in df.values]\n",
    "\n",
    "    print('Load {} done, length: {}'.format(filename, len(tuples)))\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "print(\"Start at Time: {}\".format(ctime()))\n",
    "args = torch.load('MolHF_conf.json')\n",
    "# set_random_seed(args.seed)\n",
    "# configuration\n",
    "num2atom = {0: 6, 1: 7, 2: 8, 3: 9, 4: 15, 5: 16, 6: 17, 7: 35, 8: 53}\n",
    "atom_valency = {6: 4, 7: 3, 8: 2, 9: 1, 15: 3, 16: 2, 17: 1, 35: 1, 53: 1}\n",
    "\n",
    "data_path = os.path.join('../data_preprocessed', args.dataset)\n",
    "with open(os.path.join(data_path, 'config.txt'), 'r') as f:\n",
    "    data_config = eval(f.read())\n",
    "\n",
    "with open(\"../dataset/zinc250k/moflow_idx.json\", \"r\") as f:\n",
    "    train_idx, valid_idx = json.load(f)\n",
    "dataset = PretrainDataset(\"../data_preprocessed/{}\".format(args.dataset), data_config, args)\n",
    "train_dataset = deepcopy(dataset)\n",
    "train_dataset._indices = train_idx # РґР°РЅРЅС‹Рµ С…СЂР°РЅСЏС‚СЃСЏ РІСЃРµ, РЅРѕ Р±РµСЂСѓС‚СЃСЏ С‚РѕР»СЊРєРѕ С‚Рµ, РєРѕС‚РѕСЂС‹Рµ РµСЃС‚СЊ РІ СЃРїРёСЃРєРµ РёРЅРґРµРєСЃРѕРІ\n",
    "valid_dataset = deepcopy(dataset)\n",
    "valid_dataset._indices = valid_idx # Р°РЅР°Р»РѕРіРёС‡РЅРѕ\n",
    "\n",
    "if args.hidden in ('', ','):\n",
    "    hidden = []\n",
    "else:\n",
    "    hidden = [int(d) for d in args.hidden.strip(',').split(',')]\n",
    "print('Hidden dim for output regression: ', hidden)\n",
    "\n",
    "if args.ratio in ('',','):\n",
    "    ratio = []\n",
    "else:\n",
    "    GEN_RATIO, MPROP_RATIO = [float(d) for d in args.ratio.strip(',').split(',')]\n",
    "\n",
    "if args.moduls in ('',','):\n",
    "    raise ValueError('empty moduls list')\n",
    "else:\n",
    "    moduls_list = [mod for mod in args.moduls.strip(',').split(',')]\n",
    "\n",
    "if args.moduls in ('',','):\n",
    "    raise ValueError('empty activation list')\n",
    "else:\n",
    "    acti_dict = {'tanh': nn.Tanh(), 'sigm': nn.Sigmoid(), 'relu': nn.ReLU()}\n",
    "    activ = [acti_dict[acti] for acti in args.activation.strip(',').split(',')]\n",
    "\n",
    "if args.property_model_path is None:\n",
    "    print('in')\n",
    "    mol_property = load_property_csv('../docking/DS_data/qed_dataset_all.csv')\n",
    "\n",
    "    train_dataset.is_mol_property = True\n",
    "    train_dataset.mol_property = mol_property\n",
    "    valid_dataset.is_mol_property = True\n",
    "    valid_dataset.mol_property = mol_property\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size,collate_fn=PretrainDataset.collate_fn, num_workers=args.num_workers, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size,collate_fn=PretrainDataset.collate_fn, num_workers=args.num_workers, drop_last=True)\n",
    "    \n",
    "    property_model_path = os.path.join(args.model_dir, '{}_{}-{}-{}-{}_{}.pth'.format(args.hidden, GEN_RATIO, DS_RATIO, SA_RATIO, TD_RATIO, moduls_list))\n",
    "    \n",
    "    gen_model = MolHF(data_config, args).to(args.device)\n",
    "    op.initialize_from_checkpoint(gen_model, args)\n",
    "\n",
    "    opti_model = OptimModel(gen_model, hidden, activ).to(args.device)\n",
    "    property_model = fit_model(opti_model, train_loader, valid_loader, args, property_model_path, moduls_list)   \n",
    "# else:\n",
    "#     prop_list = load_property_csv('tmp_docking_dataset.csv')\n",
    "#     train_prop = [prop_list[i] for i in train_idx]\n",
    "\n",
    "#     # DMNP\n",
    "#     dmnp_smiles = 'CC1CCC(C2=C1C=CC(=C2)C)C(C)CCC(=O)O'\n",
    "#     train_prop = [tuple(op.get_mol_property(dmnp_smiles) + [dmnp_smiles])]\n",
    "    \n",
    "#     test_prop = [prop_list[i] for i in valid_idx]\n",
    "#     property_model_path = os.path.join(args.model_dir, args.property_model_path)\n",
    "    \n",
    "#     model = MolHF(data_config, args).to(args.device)\n",
    "#     op.initialize_from_checkpoint(model, args)\n",
    "    \n",
    "#     property_model = OptimModel(model, hidden).to(args.device)\n",
    "#     property_model.load_state_dict(torch.load(property_model_path, map_location=args.device))\n",
    "\n",
    "#     property_model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
